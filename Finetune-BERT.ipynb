{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f1e3047-87a1-4957-81ed-ee096a62f654",
   "metadata": {},
   "source": [
    "# Train a BERT Model\n",
    "\n",
    "## Installation\n",
    "If you don't have python and jupyter lab already installed on you computer, install the with the following instructions\n",
    "\n",
    "[Python](https://docs.python-guide.org/starting/install3/osx/)\n",
    "\n",
    "[Jupyter Lab](https://jupyter.org/install)\n",
    "\n",
    "[Oxen AI](https://oxen.ai/?utm_source=tiktok&utm_medium=paid_social&utm_campaign=allydoesdatascience)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b191edb-9e67-43af-b53b-bb2cd5a684b2",
   "metadata": {},
   "source": [
    "# Getting your data\n",
    "\n",
    "I used [Oxen AI](https://oxen.ai/?utm_source=tiktok&utm_medium=paid_social&utm_campaign=allydoesdatascience) to find and manage my dataset for this project. \n",
    "\n",
    "IMDB Movie Reviews Dataset : [Linked Here!](https://www.oxen.ai/ox/IMDB-Movie-Reviews)\n",
    "\n",
    "Clone this repo or any other Oxen dataset repo to start your project. Follow the simple Oxen instructions to setting up your first project repo.\n",
    "\n",
    "Next, we start preprocessing. Once your repo has been cloned and you can access the data itself. I utilized the preprocessing code provided by Oxen to streamline my training even further. \n",
    "\n",
    "Navigate the notebook within your cloned repo (IMDB Movie Reviews/code/process_data.ipynb) and run the notebook as is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c59c605-14bb-4304-b569-9104d8080c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kh/jnjl8xl119q558cgvkygcs8m0000gn/T/ipykernel_14618/1675657397.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# import packages to use during training\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383ca4d6-aeac-4fb6-9b83-361809aba7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>fold</th>\n",
       "      <th>label</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_id</th>\n",
       "      <th>url</th>\n",
       "      <th>preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/test/neg/1821_4.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>neg</td>\n",
       "      <td>4</td>\n",
       "      <td>1821</td>\n",
       "      <td>http://www.imdb.com/title/tt0138541/usercomments</td>\n",
       "      <td>Alan Rickman &amp; Emma Thompson give good perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/test/neg/9487_1.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>neg</td>\n",
       "      <td>1</td>\n",
       "      <td>9487</td>\n",
       "      <td>http://www.imdb.com/title/tt0202521/usercomments</td>\n",
       "      <td>I have seen this movie and I did not care for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/test/neg/4604_4.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>neg</td>\n",
       "      <td>4</td>\n",
       "      <td>4604</td>\n",
       "      <td>http://www.imdb.com/title/tt0417658/usercomments</td>\n",
       "      <td>In Los Angeles  the alcoholic and lazy Hank Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/test/neg/2828_2.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>neg</td>\n",
       "      <td>2</td>\n",
       "      <td>2828</td>\n",
       "      <td>http://www.imdb.com/title/tt0066105/usercomments</td>\n",
       "      <td>This film is bundled along with \"Gli fumavano ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/test/neg/10890_1.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>neg</td>\n",
       "      <td>1</td>\n",
       "      <td>10890</td>\n",
       "      <td>http://www.imdb.com/title/tt0787505/usercomments</td>\n",
       "      <td>I only comment on really very good films and o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        path  fold label  rating  review_id  \\\n",
       "0   data/test/neg/1821_4.txt  test   neg       4       1821   \n",
       "1   data/test/neg/9487_1.txt  test   neg       1       9487   \n",
       "2   data/test/neg/4604_4.txt  test   neg       4       4604   \n",
       "3   data/test/neg/2828_2.txt  test   neg       2       2828   \n",
       "4  data/test/neg/10890_1.txt  test   neg       1      10890   \n",
       "\n",
       "                                                url  \\\n",
       "0  http://www.imdb.com/title/tt0138541/usercomments   \n",
       "1  http://www.imdb.com/title/tt0202521/usercomments   \n",
       "2  http://www.imdb.com/title/tt0417658/usercomments   \n",
       "3  http://www.imdb.com/title/tt0066105/usercomments   \n",
       "4  http://www.imdb.com/title/tt0787505/usercomments   \n",
       "\n",
       "                                             preview  \n",
       "0  Alan Rickman & Emma Thompson give good perform...  \n",
       "1  I have seen this movie and I did not care for ...  \n",
       "2  In Los Angeles  the alcoholic and lazy Hank Ch...  \n",
       "3  This film is bundled along with \"Gli fumavano ...  \n",
       "4  I only comment on really very good films and o...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in your processed dataset using pandas\n",
    "\n",
    "# this path may be different depending where you are in your current working directory\n",
    "# you can check using the command \"pwd\" within a notebook cell then adjust the path accordingling\n",
    "\n",
    "df = pd.read_csv(\"IMDB-Movie-Reviews/supervised.csv\") \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a056e9a-edd9-49f9-84d5-f146db851c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a train, test split using an Sklearn function\n",
    "train, test = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b00f9e2c-a744-4e0e-8567-c4e94b012c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c100e65a85114201855e901d6654bac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4750c5de1b9400195745d05e6ce7d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1059e4872d9a42d6b535b21ca389814a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly from Hugging Face \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "816aef45-6689-4348-be2e-fadf4b3b168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for encoding of the text reviews\n",
    "def convert_example_to_feature(review):\n",
    "  return tokenizer.encode_plus(review,\n",
    "                add_special_tokens = True, # add [CLS], [SEP]\n",
    "                max_length = max_length, # max length of the text that can go to BERT\n",
    "                pad_to_max_length = True, # add [PAD] tokens\n",
    "                return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1a4de64-2240-44a1-ac11-dabd430794b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyperparameters\n",
    "max_length = 512\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8107b792-b5d4-4faa-9173-edb25e9335dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to format the model output \n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "  return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1a5cc2c-5f1b-42f6-b52d-4756adbb8919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting all the functions together to complete the tokenization process\n",
    "def encode_examples(ds, limit=-1):\n",
    "  input_ids_list = []\n",
    "  token_type_ids_list = []\n",
    "  attention_mask_list = []\n",
    "  label_list = []\n",
    "  if (limit > 0):\n",
    "      ds = ds.take(limit)\n",
    "  for path, label in zip(df['path'], df['label']):\n",
    "    review = open(path, \"r\").read()\n",
    "    bert_input = convert_example_to_feature(review)\n",
    "    input_ids_list.append(bert_input['input_ids'])\n",
    "    token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "    attention_mask_list.append(bert_input['attention_mask'])\n",
    "    label_list.append([label])\n",
    "  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ceca81-75aa-46f4-a111-2543049bb980",
   "metadata": {},
   "source": [
    "Before we can can start the training process, ensure your working directory is inside the IMDB Movie Reviews Folder\n",
    "- Type the commend \"pwd\" to confirm\"\n",
    "- Use command \"cd [path_to_dir]\" to move to the correct directory\n",
    "\n",
    "Ex. cd IMDB-Movie-Reviews \n",
    "\n",
    "If you are not in the correct spot, this code will throw and error saying it can't find you data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c6a7f6e-2821-49be-8dd7-f822776961b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "ds_train_encoded = encode_examples(train).shuffle(10000).batch(batch_size)\n",
    "# test dataset\n",
    "ds_test_encoded = encode_examples(test).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87ae2f60-c1ef-4df7-a48b-1357e0763612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9841d0703aad4dafb5f890f90b1b274d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7e38d1b9da487d9f45a31e7731fd3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 2e-5\n",
    "# we will do just 1 epoch, though multiple epochs might be better as long as we will not overfit the model\n",
    "number_of_epochs = 1\n",
    "# model initialization\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "# choosing Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "483d93eb-83e0-4cb0-aaf0-02586ed92744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"This is a really good movie. I loved it and will watch again\"\n",
    "\n",
    "predict_input = tokenizer.encode(test_sentence,\n",
    "\n",
    "truncation=True,\n",
    "\n",
    "padding=True,\n",
    "\n",
    "return_tensors=\"tf\")\n",
    "\n",
    "tf_output = model.predict(predict_input)[0]\n",
    "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
    "labels = ['Negative','Positive'] #(0:negative, 1:positive)\n",
    "label = tf.argmax(tf_prediction, axis=1)\n",
    "label = label.numpy()\n",
    "print(labels[label[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03770407-9c0c-4998-8871-7367b612b317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c2e792-dad0-4932-9543-274df5800a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
